{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15. Attention Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtXtVxRcUniF"
      },
      "source": [
        "# Attention model\n",
        "\n",
        "Reference : https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
        "\n",
        "The encoder-decoder model for recurrent neural networks is an architecture for sequence-to-sequence prediction problems.\n",
        "\n",
        "**Encoder**: The encoder is responsible for stepping through the input time steps and encoding the entire sequence into a fixed length vector called a context vector.\n",
        "\n",
        "**Decoder**: The decoder is responsible for stepping through the output time steps while reading from the context vector.\n",
        "\n",
        "A problem with the architecture is that performance is poor on long input or output sequences.\n",
        "\n",
        "Attention is an extension to the architecture that addresses this limitation. It works by first providing a richer context from the encoder to the decoder and a learning mechanism where the decoder can learn where to pay attention in the richer encoding when predicting each time step in the output sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eGHsiRjWTsS",
        "outputId": "3e915bae-bab3-43ee-cee6-eeb94e5ad534"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.7.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=4dfce577dcdbaa01df4af3a8296744b2ebdf6ab20cc3ebf41df2dd43b3210450\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4zh9ZHFVSOJ"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from attention import AttentionLayer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtgjG1ofVm5x"
      },
      "source": [
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Wj9OViVqfm"
      },
      "source": [
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqwX3aQbVs9Y"
      },
      "source": [
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5ocFeh_VudB"
      },
      "source": [
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "  \n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GovSaseVzns"
      },
      "source": [
        "n_features = 50\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2z_QRxwV20M"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
        "model.add(AttentionDecoder(150, n_features))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VWtX2anV56k"
      },
      "source": [
        "# train LSTM\n",
        "for epoch in range(5000):\n",
        "\n",
        "\t# generate new random sequence\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        " \n",
        "\t# fit model for one epoch on this sequence\n",
        "\tmodel.fit(X, y, epochs=1, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVxIFwHWV9W-"
      },
      "source": [
        "# train LSTM\n",
        "for epoch in range(5000):\n",
        "\t# generate new random sequence\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t# fit model for one epoch on this sequence\n",
        "\tmodel.fit(X, y, epochs=1, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8igQwVTWDTr"
      },
      "source": [
        "# evaluate LSTM\n",
        "total, correct = 100, 0\n",
        "for _ in range(total):\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\tyhat = model.predict(X, verbose=0)\n",
        "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\tcorrect += 1\n",
        "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ijjmp83UgZ7"
      },
      "source": [
        "\n",
        "# # check some examples\n",
        "# for _ in range(10):\n",
        "# \tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "# \tyhat = model.predict(X, verbose=0)\n",
        "# \tprint('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}