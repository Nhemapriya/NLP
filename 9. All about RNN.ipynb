{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8. All about RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPvMiZ0YpktGwraxplZzMSr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"A4RXwdg0pVLs"},"source":["# Recurrent Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"d_EIR60Opax-"},"source":[" **Basics of RNN**\n","\n","*   RNN works well with sequence or range of inputs with respect to time. \n","*   Thus, it finds its wide application in NLP\n","\n","\n","**Formal Definition**\n","\n","A recurrent neural network is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence.\n","\n","\n","**Detailed explanation with a sample usecase**\n","\n","  \n","\n","*   Let's consider a NLP usecase, (ie) classification of email as (spam/ham).\n","*   The title or the subject of the email is considered as the dataset.\n","*   Let us consider a sentence X = \"you have an offer\". The sentence X can be represented as word vectors using any of the previously discussed approaches such as Bag Of Words(BOW), Word2Vec, TF-IDF. \n","*   Vectorized representation be X = <X1, X2, X3, X4>\n","\n","**Forward propagation over time**\n","\n","\n","\n","*   The input for a hidden layer for a recurrent neural network at a time instant t1 will be the vector for word X1 along with the weights W\n","*   An output o1 will be generated from the first layer.\n","*   **The key factor that distinguishes RNN from traditional ANN is that the input to the next hidden layer will be the word vector X2 along with o1 summed up together with the weights**\n","*   Thus, from this it is evident that the sequence or order is preserved as the next hidden layer input depends on the previous layer at a time instant t.\n","*   Let us consider the below image\n","\n","\n","\n","\n","![picture](https://images.deepai.org/glossary-terms/5142bdf22de24169bda948b961bfce99/download.jpeg)\n","\n","\n","\n","\n","*   For a time instant t1 or t+1, the next word vector is considered and o2 is obtained as a function of o1+w and corresponding input X2\n","*   Multiple hidden layers can be created based on this weight forwarding technique (weight recurrence)\n","*   Thus, finally a softmax loss function can be used to classify (0 and 1) as spam and ham\n","\n","\n","**Backward propagation over time**\n","\n","*   The main reason to take up backpropagation is to reduce the loss \n","*   To reduce the loss, we indeed have to update the weights\n","*   Weight updation can be done by simply taking the derivative for the original weights (using chain rule maybe) \n","*   Subtract the original weights by the derivative value and update the value\n","*   Once when the global minima is reached (zero), RNN training will stop\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uTRWjm-uyx_1"},"source":["# Problems with RNN\n","\n","\n","\n","*   In case of RNN, everytime the weights get updated based on the previous input in backpropagation\n","*   **Main issue - Vanishing gradient - In case of sigmoid function, when the derivative is found for weights, it becomes so very less that it is negligible and makes no difference in the next hidden layer's weight - does not converge**\n","\n","*  When we use any other activation function like ReLu, exploding gradient problem occurs\n","\n","*  To overcome this issue, **LSTM with RNNs are used**"]},{"cell_type":"markdown","metadata":{"id":"q11doHtM2eEC"},"source":["# LSTM Recurrent Neural Network\n","\n","Link : https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","\n","\n","Consider a usecase of text generation\n","\n","*   Say the model has generated the sentence \"My firstname is Hema\"\n","*   In this case , to generate a similar sentence and a slight difference the model has to understand that there is a change in context.\n","*   Eg: \"My surname is Priya\" . Here, the name has changed and the model needs to forget my old data and remember the new information\n","*   This is where **LSTM with RNN** comes into picture\n","\n","LSTM generally consists of four important components\n","\n","\n","\n","1.   Input gate\n","2.   Memory cell\n","3.   Forget gate\n","4.   Output gate\n","\n","\n","\n","\n","\n","*   Memory cell\n","\n","    It temporarily remembers and forgets the vectors . (ie) when the original vector is say [1, 1, 2, 1, 3], the next time when the vector is updated the vector goes like [1, 1, 0, 0, 1, 3] . Thus, information related to 3rd and 4th word is forgotten by the hidden layer\n","\n","*   Forget Gate\n","\n","    When the context is changed , the ouput vector will be changed and the previous vector will be removed or forgetted. Thus, the state of the vector is lost.\n","\n","*   Input Gate\n","\n","    Y = WX + B where X acts as the input to the hidden layer along with the added information to the memory cell\n","\n","*   Output Gate\n","\n","    All information in the memory cell is carried over back to the output layer finally (ie) memory cell + weights\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}