{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13. Bidirectional RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9qW8tYw4rwV"
      },
      "source": [
        "# Bidirectional RNN\n",
        "\n",
        "\n",
        "\n",
        "*   [All about RNN](https://github.com/Nhemapriya/NLP/blob/master/09.%20All%20about%20RNN.ipynb) Note book provides a detailed description of what RNN is and how it works\n",
        "*   To brush up once, RNN is a type of neural network wherein the next hidden layer input depends on the output of previous layer, corresponding input and the weights at an instant of time t.\n",
        "*   RNN works well with sequence of data especially words and sentences where the order in which the words appear is preserved.\n",
        "\n",
        "**Bidirectional RNN**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Let us consider a scenario where the next word in the sentence has to be predicted. Say \"I am eating -------\". It can be apples, oranges, etc..\n",
        "*   A conventional RNN architecture works in a unidirectional way where the weights are sequentially updated based on previous inputs. \n",
        "*   In bidirectional RNN , the inputs flow in the reverse direction and are combined with the normal outputs\n",
        "\n",
        "![picture](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n",
        "\n",
        "\n",
        "\n",
        "*   The above diagram can be compared with the traditional RNN architecture to obtain the difference.\n",
        "*   Additional layer of LSTM is added in the reverse direction .\n",
        "*   To determine y2, the values of y0 and y1 are required along with X2 and X3(next successive input)\n",
        "*   Since sentences are given as input to nlp tasks, It can be easily applied.\n",
        "*   It is slow and heavy when compared to LSTM-RNN \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_2wkCt88bYa"
      },
      "source": [
        "# Introduction to Sequence-to-Sequence with NN\n",
        "\n",
        "Sequence to sequence  finds its application in many of the NLP tasks where the output obtained is considered as a factor or sequence of inputs\n",
        "\n",
        "\n",
        "Eg: Consider google translate usecase where a given sentence has to be converted from english to tamil or tamil to telugu. Thus, the input and output is considered as a sequence.\n",
        "\n",
        "Eg: Consider the example of GMail where automated replies or suggestions are provided to respond back (Thank you, get back to you soon)\n",
        "\n",
        "**Architecture of Sequence to sequence**\n",
        "\n",
        "![picture](https://miro.medium.com/max/1400/1*Ismhi-muID5ooWf3ZIQFFg.png)\n",
        "\n",
        "The major components of sequence to sequence architecture are **encoders** and **decoders** \n",
        "\n",
        "**Encoders** can generally be LSTMs or any type of RNN , except for the fact that  only the last layer has the output. In case of language translation, <EOS> End of sentence output is only present. The output from the encoder can be called as context vector \n",
        "\n",
        "**Decoders** - takes in the context output from the encoder and reproduces the text until the <EOS> is obtained. Every layer in decoder will produce a vector output that might correspond to a particular word\n",
        "\n",
        "Eg: X (Input) - <X1, X2, X3.....Xt> - ABC\n",
        "\n",
        "    corresponding output Y - <y1, y2, y3... yt> - XYZ.\n",
        "\n",
        "\n",
        "This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) -- in general, it is applicable any time you need to generate text.\n",
        "\n"
      ]
    }
  ]
}